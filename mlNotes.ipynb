{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlNotes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J37s3IucASXN"
      },
      "source": [
        "# Notes\n",
        "\n",
        "~ normie stuff\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ3kudBsASUY"
      },
      "source": [
        "### Machine Learning Yearning by Andrew Ng:\n",
        "\n",
        "1. Generally as data increases performance increases if model is good, this rate of increases in this fashion: <br>\n",
        "Traditional ML Algo < Small NN < Medium NN < Large NN (* traditional might outdo in some less data cases if well hand engineered solutions are used)\n",
        "\n",
        "2. Try to get dev set and test set data from the same distribution as possible, this ensures divergence in results between the two is only due to model overfitting to dev set\n",
        "\n",
        "3. Always aim to develop a single number evaluation metric, e.g. precision and recall can be replaced with f1-score\n",
        "\n",
        "4. If single number metric is not suitable consider splitting into satisficing metrics and optimizing metric and the combining the optimizing metrics while setting goals for satisficing metrics.\n",
        "\n",
        "5. Test set is different from dev set in the sense that is results are not analysed in the iterative techincal development, thus used rarely. That being said its normal to replace it from time to time.\n",
        "\n",
        "6. Error analysis is the process of analysing the data where your model went wrong. Its useful but biased. \n",
        "\n",
        "7. During error analysis maintain a spreadsheet with categories of possible error inducing causes. These may overlap. Keep a track of the major contributor and start solving them first. Cost to Benefit analysis for each is a good way to go.\n",
        "\n",
        "8. If too much data to even do error analysis (errors are generally minority thus smaller set), then split your dev into eyeball dev and blackbox dev. Size of eyeball dev can be set according to your manual error analysis appetite.\n",
        "\n",
        "9. Bias and variance have formal statistical definition but in general ml, bias is error of model predictions within training set and variance is difference in training and dev errorr.\n",
        "\n",
        "10. low bias high variance is overfitting. high bias low variance is underfitting. both high is difficult problem to analyse.\n",
        "\n",
        "11. Your goal for your bias might not be 0%. This goal for bias is optimal error rate or bayes error rate. Only the avoidable bias accounts for underfitting. \n",
        "\n",
        "12. to improve bias: increase model size, increase/ improve features, decrease regularization, modify architecture\n",
        "\n",
        "13. to improve variance: add more training data, add regularization, add early stopping, reduce features, make model smaller, change architecture\n",
        "\n",
        "14. Learning curves give a lot of insight into decision making in model and corpus development. e.g. Learning has saturated wrt data size, then more data wont do anything. \n",
        "\n",
        "15. Tips for making curves:\n",
        "- too less data thus too much noise in early data sizes. Use random sampling with replacement and average out the output\n",
        "- too much data to try out all linearly spaced data sizes, try a lograthmically spaced one.\n",
        "\n",
        "16. If you are using data outside of test distribution for increasing generalization of model, be careful about not using a lot of it in dev and test. Keep dev and test clean.\n",
        "\n",
        "17. One way of dealing with multi distribution data is having a customized loss function, with different one for each distribution according to its quality weightage.\n",
        "\n",
        "18. If you have a 'mismatch' between your training and dev sets due to pre-discussed distribution discrepancies, consider having two dev sets. One from training distribution and one from test distribution. Now with bias and variance analysis you get a better picture for distribution vs model shortcomings.\n",
        "\n",
        "19. Data synthesis is great for creating new data, but is full of bias if not taken care of.\n",
        "\n",
        "20. There are two aspects of ML: cost function and search method (MSE and SGD). For a model that is underfitting, how to check which one is the problem? Take 'misclassified cases' from the dev set and check score of true solution vs that of predicted solution. if score of true solution is higher loss funciton is ok but search is not, else vice versa. Basically always check the reward(true) vs reward(prediction) to understand if algorithm is missing the optima (search issue) or the optima itself is not good enough (loss issue). This is called optimization verification test.\n",
        "\n",
        "21. since all ml models are DAGs and multi model pipelines are DAGs as well, any ML model is a DAG and thus can be analysed as one: from left to right. For example: Model1 --> Model2 --> Model3 is giving subpar ops, so you remove model1 and feed model2 perfect output as expected form model1, does this give good o/p then model1 was at fault, if not then continue with model2 similarly.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1W5Gs_yAWkF"
      },
      "source": [
        "### Hand's on Machine Learning by Geron\n",
        "\n",
        "#### Chapter1:\n",
        "\n",
        "1. Supervised learning:has label, used mostly for classifiers and predictors(regression). Attribute is a data catagory, feature is an attribute and its value (can be used interchanagbly, meh). \n",
        "\n",
        "2. Unsupervised learning: no labels, used for custering (geagraphical classification), visualization algorithms, dimensionality reduction, anomlay/novelty detection, associative learning. If you use clustering and some available labeled data to train, its semisupervised.\n",
        "\n",
        "3. Reinforcement learning: There is an agent with rules and action predefined, and it performs in a environment. Learning is based on reward function of the enivronment. The output here is the final policy the agent generated.\n",
        "\n",
        "4. Offline/batch learning is when model is trained using fresh new batches of data. Online learning is where new data is used to incrementally update the model. Online learning methods used to train a offline model with a huge dataset (thus couldnt be loaded onto the memory at once) in mini batches is known as outofcore learning. In online learning, learning rate also determines sensitivity to recent data, which may be useful in some models e.g. financial data.\n",
        "\n",
        "5. Instance based learning is when model only uses some instances of data to make a prediction for a specific input (KNN), model based learning is where all instances contribute to the prediction of any input (Linear regression).\n",
        "\n",
        "6. ML also has drawback: Sampling Bias (training data generally is never representative of field data), sensitivity to data quality, and most importantly 'unreasonable effectiveness of data' which is the phenomenon noticed that the quality/quantity of data matters way more than that of the model (simple models converged to big models when data was abundant)\n",
        "\n",
        "7. Training set: Data used to train, Validation/Development set: Data used to make technical decisions (hyperparam tuning, regularization, etc), test set: final decision regarding current version.\n",
        "\n",
        "8. This split also has a few issues, dev overfitting, inconsistency of dev error because of less data. To solve this use cross-validation, that is training set is randomly split into dev and training set multiple times and trained and errors are calculated as an average of all iterations. \n",
        "\n",
        "9. When training data is enlarged with data (of poorer quality or of outside required distribution) and then you have variance, you now cant just blame overfitting, but also mismatch (between training set and dev set distribution). To deal with this just split new training set into two parts, training and training dev (test dev and test set are seperate). This will allow you to attribute errors more precisesly to or overfitting. Basically splitting is done to check overfitting to a specific data distribution.\n",
        "\n",
        "#### Chapter 2:\n",
        "\n",
        "1. 'm' is generally used to denote the number of samples currently being evaluated (training size while training, dev size while validating), basically number of samples you are calculating rmse for.\n",
        "\n",
        "2. x(i) is a vector of input features, y(i) is its corresponding label\n",
        "\n",
        "3. X is the input matrix X = [x(0)T   x(1)T  ...] T\n",
        "\n",
        "4. 'h' is the hypothesis/model. So the prediction of the model for the ith input is h(x(i)), or more commonly written as $\\hat{y(i)}$, thus the error is y(i) - $\\hat{y(i)}$\n",
        "\n",
        "5. Cost function is basically applied on X for hypothesis h, e.g. RMSE(X,h)\n",
        "\n",
        "6. Generally lowercase normal letters are used for scalars, y(i),lowercase bold letters for vectors **x(i)**, and uppercase letters for matrices X.\n",
        "\n",
        "7. Cost functions basically penalize the hypothesis based on distance between predicted vector and target vector (in vector world this distance is called Norm), thus RMSE is sometimes called as L2 Norm or ||.||2, MAE is called L1 norm or ||.||1\n",
        "\n",
        "8. In generall a Lk norm is more sensitive to outliers as k increases. Thus if you have lesser outliers (say, a bell curve), use L2 (RMSE). Linfinity basically means you have raised every error term (loss term) to infinity, this means the mean absolute maximum loss is the cost. where L0 is where all errors are 1, and thus cost is constant and independent on any losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X3gyWcmAVy1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}