{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4eb8c7",
   "metadata": {},
   "source": [
    "# JAX Notes\n",
    "---\n",
    "My notes when I explore JAX. \n",
    "\n",
    "## Contents\n",
    "**1. Basics:**<br>\n",
    " - Function transformations and numpy.\n",
    " - Training with and without - batches & jit.\n",
    " \n",
    "**2. Quickstart:**<br>\n",
    " - Keys and random numbers.\n",
    " - Asynchronous dispatch.\n",
    " - `jit`, `grad` & `vmap`.\n",
    " \n",
    "**3. Neural Networks and Data:**<br>\n",
    " - Jax does not have dataloaders and datasets.\n",
    " - Training and evaluating a neural network.\n",
    "\n",
    "**4. Jax: The sharp bits:**<br>\n",
    " - Functional nature and in-place operations.\n",
    " - Conditionals (yet to make)\n",
    " - Loops & scan (yet to make)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42ec7e",
   "metadata": {},
   "source": [
    "## 1. Basics\n",
    "---\n",
    "Main reference: https://colinraffel.com/blog/you-don-t-know-jax.html\n",
    "\n",
    "On a high level, jax is numpy made to interface with specialized parallel computing hardware for ML purposes. At least, jax, at its best hopes to achieve numpy level comfort with users.\n",
    "\n",
    "So what does it have that numpy doesn't?<br>\n",
    "    1. `jit` - Just in time compilation --> Optimized compilation to enhance speed. Makes function computation faster.<br>\n",
    "    2. `grad` - Gradients --> Some way to enhance methods of producing gradients of any function outputs w.r.t. chosen params.<br>\n",
    "    3. `vmap` - Vectorization --> Batching of inputs into function to be executed in parallel.<br>\n",
    "    \n",
    "To neatly incorporate these features, jax operates in a functional paradigm and appropriately calls the above modifications as 'transformations'. \n",
    "\n",
    "Yes, jax expects the functions the above transformations operate on to be `pure functions'. This means, the user must be careful with python's procedural-ness and dynamic-ness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00eba272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # to generate random numbers\n",
    "import itertools # to count\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np #allows previous numpy code to be directly interchanged with JAX\n",
    "import numpy as onp\n",
    "#NOTE: The convention of importing JAX is still divisive -->\n",
    "# JAX centric new codes import jax.numpy as jnp and numpy as np\n",
    "# But old codes with numpy that are now expected to work on jax just edit the numpy to jax.numpy\n",
    "# onp is for original numpy\n",
    "\n",
    "#More appropriate imports\n",
    "import jax.numpy as jnp #allows previous numpy code to be directly interchanged with JAX\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67052d1",
   "metadata": {},
   "source": [
    "Chosen example: One of the core problems of AGI is learning the Exclusive OR (XOR) function with a neural network. Based on the book Perceptron by Minsky and Papert, who claimed XOR was not solvable with 2 layer feedforward layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564190d",
   "metadata": {},
   "source": [
    "### Net & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb4942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid nonlinearity - usual code\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# Computes our network's output - w1\n",
    "def net(params, x):\n",
    "    w1, b1, w2, b2 = params #w1 - 3x2, b1 - 3x1, w2 - 3x1, b2 - 1x1\n",
    "    hidden = jnp.tanh(jnp.dot(w1, x) + b1) #output of hidden layer\n",
    "    return sigmoid(jnp.dot(w2, hidden) + b2) #output layer\n",
    "\n",
    "# Cross-entropy loss\n",
    "def loss(params, x, y):\n",
    "    out = net(params, x)\n",
    "    cross_entropy = -y * jnp.log(out) - (1 - y)*jnp.log(1 - out)\n",
    "    return cross_entropy\n",
    "\n",
    "# Utility function for testing whether the net produces the correct\n",
    "# output for all possible inputs\n",
    "def test_all_inputs(inputs, params):\n",
    "    predictions = [int(net(params, inp) > 0.5) for inp in inputs]\n",
    "    for inp, out in zip(inputs, predictions):\n",
    "        print(inp, '->', out)\n",
    "    return (predictions == [np.bitwise_xor(*inp) for inp in inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f343d",
   "metadata": {},
   "source": [
    "### Training - no batches, one sample at a time, without JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7248ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 time taken: 0.3074319362640381 s\n",
      "[0 0] -> 1\n",
      "[0 1] -> 1\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 10 time taken: 0.004792213439941406 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 20 time taken: 0.004876852035522461 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 30 time taken: 0.004856109619140625 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 40 time taken: 0.0047109127044677734 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 50 time taken: 0.004727840423583984 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 60 time taken: 0.0046808719635009766 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 70 time taken: 0.004862070083618164 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 80 time taken: 0.005316019058227539 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "____________________________________________________________________________________________________\n",
      "Total time taken: 0.8159868717193604 s\n"
     ]
    }
   ],
   "source": [
    "def initial_params():\n",
    "    return [\n",
    "        np.random.randn(3, 2),  # w1\n",
    "        np.random.randn(3),  # b1\n",
    "        np.random.randn(3),  # w2\n",
    "        np.random.randn(),  #b2\n",
    "    ]\n",
    "\n",
    "loss_grad = jax.grad(loss) #loss is a function, loss_grad is also a function\n",
    "\n",
    "# Stochastic gradient descent learning rate\n",
    "learning_rate = 1.\n",
    "# All possible inputs\n",
    "inputs = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Initialize parameters randomly\n",
    "params = initial_params()\n",
    "\n",
    "startfull = time.time()\n",
    "for n in itertools.count(): #just counts, consider using while?\n",
    "    start = time.time()\n",
    "    # Grab a single random input, notice x is jax array\n",
    "    x = inputs[np.random.choice(inputs.shape[0])] #randomly gives integer in range.  \n",
    "    #True stochastic gradient descent. Since only one sample per cycle\n",
    "    # Compute the target output\n",
    "    y = np.bitwise_xor(*x)\n",
    "    \n",
    "    # Get the gradient of the loss for this input/output pair\n",
    "    grads = loss_grad(params, x, y) #autogradient for your function, but again, only one input at a time\n",
    "    # Update parameters via gradient descent\n",
    "    params = [param - learning_rate * grad\n",
    "              for param, grad in zip(params, grads)]\n",
    "    # Every 100 iterations, check whether we've solved XOR\n",
    "    if not n % 10:\n",
    "        end = time.time()\n",
    "        print('Iteration {}'.format(n),'time taken:', end-start,'s')\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break\n",
    "endfull = time.time()\n",
    "print('_'*100)\n",
    "print('Total time taken:', endfull-startfull,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66caa33f",
   "metadata": {},
   "source": [
    "### Training - no batches, one sample at a time, with JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4d0d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 time taken: 0.03687405586242676 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 10 time taken: 0.00014495849609375 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "Iteration 20 time taken: 0.00015091896057128906 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration 30 time taken: 0.00013494491577148438 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "Iteration 40 time taken: 0.00012969970703125 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "Iteration 50 time taken: 0.00012683868408203125 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "Iteration 60 time taken: 0.00013303756713867188 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "____________________________________________________________________________________________________\n",
      "Total time taken: 0.05040621757507324 s\n"
     ]
    }
   ],
   "source": [
    "loss_grad = jax.jit(jax.grad(loss))#loss is a function, loss_grad is also a function\n",
    "# Stochastic gradient descent learning rate\n",
    "learning_rate = 1.\n",
    "# All possible inputs\n",
    "inputs = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Initialize parameters randomly\n",
    "params = initial_params()\n",
    "\n",
    "startfull = time.time()\n",
    "for n in itertools.count(): #just counts, consider using while?\n",
    "    start = time.time()\n",
    "    # Grab a single random input, notice x is jax array\n",
    "    x = inputs[np.random.choice(inputs.shape[0])] #randomly gives integer in range.  \n",
    "    #True stochastic gradient descent. Since only one sample per cycle\n",
    "    # Compute the target output\n",
    "    y = np.bitwise_xor(*x)\n",
    "    \n",
    "    # Get the gradient of the loss for this input/output pair\n",
    "    grads = loss_grad(params, x, y) #autogradient for your function, but again, only one input at a time\n",
    "    # Update parameters via gradient descent\n",
    "    params = [param - learning_rate * grad\n",
    "              for param, grad in zip(params, grads)]\n",
    "    # Every 100 iterations, check whether we've solved XOR\n",
    "    if not n % 10:\n",
    "        end = time.time()\n",
    "        print('Iteration {}'.format(n),'time taken:', end-start,'s')\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break\n",
    "endfull = time.time()\n",
    "print('_'*100)\n",
    "print('Total time taken:', endfull-startfull,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47245570",
   "metadata": {},
   "source": [
    "### Training - batching and JIT - mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9735b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 time taken: 0.18471884727478027 s\n",
      "[0 0] -> 1\n",
      "[0 1] -> 0\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "Iteration 10 time taken: 0.0007569789886474609 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration 20 time taken: 0.000438690185546875 s\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "____________________________________________________________________________________________________\n",
      "Total time taken: 0.2761819362640381 s\n"
     ]
    }
   ],
   "source": [
    "loss_grad = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0), out_axes=0))\n",
    "# --------Just in time--parallel----batching over?-params, x[0], y[0]---O[0] --> 0th axis batching\n",
    "params = initial_params()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "startfull = time.time()\n",
    "for n in itertools.count():\n",
    "    start = time.time()\n",
    "    \n",
    "    # Generate a batch of inputs\n",
    "    x = inputs[np.random.choice(inputs.shape[0], size=batch_size)] #(batch_size (parallelization along here), feature_length) = (100,2)\n",
    "    y = np.bitwise_xor(x[:, 0], x[:, 1]) #only one axis\n",
    "    \n",
    "    # The call to loss_grad remains the same!\n",
    "    grads = loss_grad(params, x, y)\n",
    "    \n",
    "    # Note that we now need to average gradients over the batch\n",
    "    params = [param - learning_rate * np.mean(grad, axis=0)\n",
    "              for param, grad in zip(params, grads)] #only difference is mean gradient for mini-batch\n",
    "    if not n % 10:\n",
    "        end = time.time()\n",
    "        print('Iteration {}'.format(n),'time taken:', end-start,'s')\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break\n",
    "endfull = time.time()\n",
    "print('_'*100)\n",
    "print('Total time taken:', endfull-startfull,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683b374",
   "metadata": {},
   "source": [
    "Batching allows more stable learning, lesser chance of divergence or incorrect minima. As seen, vectorization did not increase the per iteration time much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f500d9",
   "metadata": {},
   "source": [
    "## 2. Quickstart\n",
    "---\n",
    "Main reference: https://jax.readthedocs.io/en/latest/notebooks/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0a35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp \n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ebeea",
   "metadata": {},
   "source": [
    "Since jax operates in functional paradigm, functions can't retain any state. This means keys need to be explicitly provided each function call. This is where `jax.random.split` comes in. It allows the user to 'thread' the key through each function call and maintain repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec2b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38812608 -0.04487164 -2.0427258   0.07932311  0.33349916  0.7959976\n",
      " -1.4411978  -1.6929979  -0.37369204 -1.5401139 ] [-0.08943313  1.5241055  -0.41166604  0.8208107   1.0238154   0.4196539\n",
      "  1.0200214   0.00804093  0.63066274  1.7859513 ]\n"
     ]
    }
   ],
   "source": [
    "#creating random numbers\n",
    "#one of the few things where jax and numpy defer in syntax\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "key, use_key = random.split(key)\n",
    "x = random.normal(use_key, (10,))\n",
    "\n",
    "key, use_key = random.split(key)\n",
    "y = random.normal(use_key, (10,))\n",
    "\n",
    "print(x , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345cfd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.9 µs ± 185 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "size = 30\n",
    "key, use_key = random.split(key)\n",
    "x = random.normal(use_key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU\n",
    "\n",
    "#We added that block_until_ready because JAX uses asynchronous execution by default (see {ref}async-dispatch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77259d41",
   "metadata": {},
   "source": [
    "**Asynchronous Dispatch**\n",
    "\n",
    "When an operation such as `jnp.dot(x, x)` is executed, JAX does not wait for the operation to complete before returning control to the Python program. Instead, JAX returns a `DeviceArray` value, which is a future, i.e., a value that will be produced in the future on an accelerator device but isn’t necessarily available immediately. We can inspect the `shape` or `type` of a DeviceArray without waiting for the computation that produced it to complete, and we can even pass it to another JAX computation, as we do with the addition operation here. Only if we actually inspect the value of the array from the host, for example by printing it or by converting it into a plain old `numpy.ndarray` will JAX force the Python code to wait for the computation to complete.\n",
    "\n",
    "This is called asynchronous dispatch. \n",
    "\n",
    "Why is it useful?<br>\n",
    "It allows the program to run ahead of an accelerator device, \"in the case where the host program does not actually need to inspect the output of the specific accelerator computation\". This allows the program enqueue arbitrary amounts of work on the accelerator and make more efficient use of its time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb150d",
   "metadata": {},
   "source": [
    "**NOTE: The below exercise will only make sense with a GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d3484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.7 ms, sys: 1.09 ms, total: 40.8 ms\n",
      "Wall time: 12.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[255.01973, 246.64864, 254.13373, ..., 233.67952, 247.68939,\n",
       "        238.36853],\n",
       "       [262.65982, 253.28915, 259.18246, ..., 239.03183, 253.16756,\n",
       "        249.44124],\n",
       "       [259.38916, 252.72754, 258.2306 , ..., 237.83559, 252.41093,\n",
       "        246.62468],\n",
       "       ...,\n",
       "       [256.1581 , 250.092  , 254.72174, ..., 239.23874, 247.72684,\n",
       "        244.16638],\n",
       "       [268.22662, 258.91205, 262.33398, ..., 245.26648, 259.0539 ,\n",
       "        258.337  ],\n",
       "       [254.16138, 251.7543 , 256.083  , ..., 238.59848, 245.62595,\n",
       "        240.22354]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random.uniform(random.PRNGKey(0), (1000, 1000))\n",
    "%time jnp.dot(x, x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd9762",
   "metadata": {},
   "source": [
    "Wall time is surprisingly low, because the time was estimated not for the computation but estimated for the dispatch to accelerator. To check actual time, perform a inspection level task (convert to numpy array) or `block_until_ready()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c49b0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.6 ms, sys: 2.21 ms, total: 38.8 ms\n",
      "Wall time: 6.62 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[255.01973, 246.64864, 254.13373, ..., 233.67952, 247.68939,\n",
       "        238.36853],\n",
       "       [262.65982, 253.28915, 259.18246, ..., 239.03183, 253.16756,\n",
       "        249.44124],\n",
       "       [259.38916, 252.72754, 258.2306 , ..., 237.83559, 252.41093,\n",
       "        246.62468],\n",
       "       ...,\n",
       "       [256.1581 , 250.092  , 254.72174, ..., 239.23874, 247.72684,\n",
       "        244.16638],\n",
       "       [268.22662, 258.91205, 262.33398, ..., 245.26648, 259.0539 ,\n",
       "        258.337  ],\n",
       "       [254.16138, 251.7543 , 256.083  , ..., 238.59848, 245.62595,\n",
       "        240.22354]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time jnp.dot(x, x).block_until_ready() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa54d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.1 µs ± 289 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import device_put\n",
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a25a88",
   "metadata": {},
   "source": [
    "The output of `jax.device_put` still acts like an NDArray, but it only copies values back to the CPU when they're needed for printing, plotting, saving to disk, branching, etc. The behavior of `jax.device_put` is equivalent to the function `jit(lambda x: x)`, but it's faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04064fd",
   "metadata": {},
   "source": [
    "**Using `jax.jit` to speed up functions**\n",
    "\n",
    "JAX runs transparently on the GPU or TPU (falling back to CPU if you don't have one). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, we can use the `@jit` decorator to compile multiple operations together using [XLA](https://www.tensorflow.org/xla). Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f892353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.55 ms ± 139 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "485a0931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 µs ± 11.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a975445",
   "metadata": {},
   "source": [
    "This means the first time selu is called, it will be jit compiled and then cached for other use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a41816",
   "metadata": {},
   "source": [
    "**Using `jax.grad` for finding derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e6ebff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661197 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7adef926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661196 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "# to verify\n",
    "def derivative_sum_log(x):\n",
    "    return jnp.exp(-x)/ (1.0 + jnp.exp(-x))**2\n",
    "print(derivative_sum_log(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4157919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0353256\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b909f6e",
   "metadata": {},
   "source": [
    "For more advanced autodiff, you can use `jax.vjp` for reverse-mode vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here's one way to compose them to make a function that efficiently computes full Hessian matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72c75668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "    return jit(jacfwd(jacrev(fun)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c49e01",
   "metadata": {},
   "source": [
    "**Auto-vectorization with `jax.vmap`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ea388",
   "metadata": {},
   "source": [
    "`~jax.vmap` is the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a function’s primitive operations for better performance. When composed with {func}`~jax.jit`, it can be just as fast as adding the batch dimensions by hand.\n",
    "\n",
    "This means that instead of just applying `func(var) for var in mapped(vars)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "207c1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "    return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f294c",
   "metadata": {},
   "source": [
    "How would you apply the `apply_matrix` function to each of the 10 vectors in batched_x?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0b7ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "401 µs ± 2.58 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "    return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7db254ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "15.8 µs ± 118 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "    return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75ef2e",
   "metadata": {},
   "source": [
    "## 3. Neural Networks and Data-loading\n",
    "---\n",
    "Main reference: https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb\n",
    "\n",
    "Implementing a simple neural network without using specialized libraries (like Haiku, Flax or Equinox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fcb9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0989e",
   "metadata": {},
   "source": [
    "Important to note that any library that works with numpy to make networks can be used, but in this notebook things are kept simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "163b0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "\n",
    "\n",
    "def random_layer_params(m: 'inputdim', n: 'outputdim', key, scale=1e-2): \n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "# Waav\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "step_size = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f92a6",
   "metadata": {},
   "source": [
    "**Auto-batching predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96bcc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "# a function writtine considering\n",
    "# only one image at a time \n",
    "def predict(params, image):\n",
    "    # per-example predictions\n",
    "    activations = image\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3aac82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# This works on single examples\n",
    "random_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\n",
    "preds = predict(params, random_flattened_image)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c373ca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid shapes!\n"
     ]
    }
   ],
   "source": [
    "# Doesn't work with a batch\n",
    "random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28)) # 10 images at once\n",
    "try:\n",
    "    preds = predict(params, random_flattened_images)\n",
    "except TypeError:\n",
    "    print('Invalid shapes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f3f1998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Let's upgrade it to handle batches using `vmap`\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0)) # we want the first dim to be parallelized (784)\n",
    "\n",
    "\n",
    "# `batched_predict` has the same call signature as `predict`\n",
    "batched_preds = batched_predict(params, random_flattened_images)\n",
    "print(batched_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "522ae6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "def accuracy(params, images, targets):\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    preds = batched_predict(params, images)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ff52aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavrao/miniconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-12 15:43:39.490964: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to ./data/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 100%|██████████| 5/5 [00:00<00:00,  6.28 file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to ./data/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "data_dir = './data/'\n",
    "\n",
    "# Fetch full datasets for evaluation\n",
    "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
    "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, \n",
    "                             with_info=True)\n",
    "mnist_data = tfds.as_numpy(mnist_data)\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "num_labels = info.features['label'].num_classes\n",
    "h, w, c = info.features['image'].shape\n",
    "num_pixels = h * w * c\n",
    "\n",
    "# Full train set\n",
    "train_images, train_labels = train_data['image'], train_data['label']\n",
    "train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
    "train_labels = one_hot(train_labels, num_labels)\n",
    "\n",
    "# Full test set\n",
    "test_images, test_labels = test_data['image'], test_data['label']\n",
    "test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
    "test_labels = one_hot(test_labels, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b57f46be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:43:45.709673: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 2.24 sec\n",
      "Training set accuracy 0.92535001039505\n",
      "Test set accuracy 0.9269999861717224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:43:48.636377: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 in 2.00 sec\n",
      "Training set accuracy 0.9428166747093201\n",
      "Test set accuracy 0.9412999749183655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:43:51.067198: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 in 2.05 sec\n",
      "Training set accuracy 0.9532833695411682\n",
      "Test set accuracy 0.95169997215271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:43:53.423913: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 in 1.96 sec\n",
      "Training set accuracy 0.9601166844367981\n",
      "Test set accuracy 0.9555999636650085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:43:55.755061: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 in 1.94 sec\n",
      "Training set accuracy 0.9652000069618225\n",
      "Test set accuracy 0.9603999853134155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:43:58.075322: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 in 1.95 sec\n",
      "Training set accuracy 0.9692167043685913\n",
      "Test set accuracy 0.9629999995231628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:44:00.372338: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 in 1.91 sec\n",
      "Training set accuracy 0.9726166725158691\n",
      "Test set accuracy 0.9656999707221985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:44:02.749534: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 in 2.01 sec\n",
      "Training set accuracy 0.975433349609375\n",
      "Test set accuracy 0.9666000008583069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:44:05.074357: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 in 1.94 sec\n",
      "Training set accuracy 0.9780166745185852\n",
      "Test set accuracy 0.9680999517440796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:44:07.403983: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 in 1.93 sec\n",
      "Training set accuracy 0.9801999926567078\n",
      "Test set accuracy 0.9688999652862549\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 10\n",
    "\n",
    "def get_train_batches():\n",
    "    # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
    "    ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n",
    "    # You can build up an arbitrary tf.data input pipeline\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
    "    return tfds.as_numpy(ds)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for x, y in get_train_batches():\n",
    "        x = jnp.reshape(x, (len(x), num_pixels))\n",
    "        y = one_hot(y, num_labels)\n",
    "        params = update(params, x, y)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b72bd",
   "metadata": {},
   "source": [
    "# 4. Jax - The Sharp Bits\n",
    "---\n",
    "https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.ipynb#scrollTo=cDpQ5u63Ba_H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf8240",
   "metadata": {},
   "source": [
    "Some under-the-hood tidbits about jax that are good to know as you can not just rely on errors. JAX is made to operate just like numpy other than the extra transformations. But JAX's approach causes it to deviate sometimes, this can cause issues if not addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e8e94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jax import grad, jit\n",
    "from jax import lax\n",
    "from jax import random\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e8cde",
   "metadata": {},
   "source": [
    "**1. JAX's functional approach**\n",
    "\n",
    "JAX's jit optimizes compilation but expects the functions to be pure. A functionally pure function does not depend on a global state and gives the same output every time for the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e64b69e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing function\n",
      "First call:  4.0\n",
      "Second call:  5.0\n",
      "Executing function\n",
      "Third call, different type:  [5.]\n"
     ]
    }
   ],
   "source": [
    "def impure_print_side_effect(x):\n",
    "    print(\"Executing function\")  # This is a side-effect\n",
    "    return x\n",
    "\n",
    "# The side-effects appear during the first run\n",
    "print (\"First call: \", jit(impure_print_side_effect)(4.))\n",
    "\n",
    "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
    "# This is because JAX now invokes a cached compilation of the function\n",
    "print (\"Second call: \", jit(impure_print_side_effect)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "print (\"Third call, different type: \", jit(impure_print_side_effect)(jnp.array([5.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24fff3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:  4.0\n",
      "Second call:  5.0\n",
      "Third call, different type:  [14.]\n"
     ]
    }
   ],
   "source": [
    "g = 0.\n",
    "def impure_uses_globals(x):\n",
    "    return x + g\n",
    "\n",
    "# JAX captures the value of the global during the first run\n",
    "print (\"First call: \", jit(impure_uses_globals)(4.))\n",
    "g = 10.  # Update the global\n",
    "\n",
    "# Subsequent runs may silently use the cached value of the globals\n",
    "print (\"Second call: \", jit(impure_uses_globals)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "# This will end up reading the latest value of the global\n",
    "print (\"Third call, different type: \", jit(impure_uses_globals)(jnp.array([4.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "727ebebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:  4.0\n",
      "Saved global:  Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    }
   ],
   "source": [
    "g = 0.\n",
    "def impure_saves_global(x):\n",
    "    global g\n",
    "    g = x\n",
    "    return x\n",
    "\n",
    "# JAX runs once the transformed function with special Traced values for arguments\n",
    "print (\"First call: \", jit(impure_saves_global)(4.))\n",
    "print (\"Saved global: \", g)  # Saved global has an internal JAX value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e998b",
   "metadata": {},
   "source": [
    "**NOTE**: A Python function can be functionally pure even if it actually uses stateful objects internally, as long as it does not read or write external state. So its fine to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef3c3dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "def pure_uses_internal_state(x):\n",
    "    state = dict(even=0, odd=0)\n",
    "    for i in range(10):\n",
    "        state['even' if i % 2 == 0 else 'odd'] += x\n",
    "    return state['even'] + state['odd']\n",
    "\n",
    "print(jit(pure_uses_internal_state)(5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce475f",
   "metadata": {},
   "source": [
    "It is not recommended to use iterators in any JAX function you want to `jit` or in any control-flow primitive. The reason is that an iterator is a python object which introduces state to retrieve the next element. Therefore, it is incompatible with JAX functional programming model. In the code below, there are some examples of incorrect attempts to use iterators with JAX. Most of them return an error, but some give unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68fb6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "from jax import make_jaxpr\n",
    "\n",
    "# lax.fori_loop\n",
    "array = jnp.arange(10)\n",
    "print(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\n",
    "iterator = iter(range(10))\n",
    "print(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0\n",
    "\n",
    "# lax.scan\n",
    "def func11(arr, extra):\n",
    "    ones = jnp.ones(arr.shape)\n",
    "    def body(carry, aelems):\n",
    "        ae1, ae2 = aelems\n",
    "        return (carry + ae1 * ae2 + extra, carry)\n",
    "    return lax.scan(body, 0., (arr, ones))\n",
    "make_jaxpr(func11)(jnp.arange(16), 5.)\n",
    "# make_jaxpr(func11)(iter(range(16)), 5.) # throws error\n",
    "\n",
    "# lax.cond\n",
    "array_operand = jnp.array([0.])\n",
    "lax.cond(True, lambda x: x+1, lambda x: x-1, array_operand)\n",
    "iter_operand = iter(range(10))\n",
    "# lax.cond(True, lambda x: next(x)+1, lambda x: next(x)-1, iter_operand) # throws error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84451068",
   "metadata": {},
   "source": [
    "**2. JAX's immutability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8121cb",
   "metadata": {},
   "source": [
    "In numpy we can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02a98037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original array:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "updated array:\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array = np.zeros((3,3), dtype=np.float32)\n",
    "print(\"original array:\")\n",
    "print(numpy_array)\n",
    "\n",
    "# In place, mutating update\n",
    "numpy_array[1, :] = 1.0\n",
    "print(\"updated array:\")\n",
    "print(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ec45d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Minimal\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mTypeError\u001b[0m\u001b[0;31m:\u001b[0m '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
     ]
    }
   ],
   "source": [
    "%xmode Minimal\n",
    "jax_array = jnp.zeros((3,3), dtype=jnp.float32)\n",
    "\n",
    "# In place update of JAX's array will yield an error!\n",
    "jax_array[1, :] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a78f0",
   "metadata": {},
   "source": [
    "JAX not allowing mutability is to avoid issues with its functional approach. Array updates in JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65349e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated array:\n",
      " [[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "updated_array = jax_array.at[1, :].set(1.0)\n",
    "print(\"updated array:\\n\", updated_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd75255",
   "metadata": {},
   "source": [
    "**NOTE:** JAX's updating is out-of-place to preserve functional-ness. New array, new pointer. But it does optimize and update in-place in a `jit` context to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97a0c72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original array unchanged:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"original array unchanged:\\n\", jax_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43bf52",
   "metadata": {},
   "source": [
    "Index-based operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a52c826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original array:\n",
      "[[1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]]\n",
      "new array post-addition:\n",
      "[[1. 1. 1. 8. 8. 8.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 8. 8. 8.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 8. 8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"original array:\")\n",
    "jax_array = jnp.ones((5, 6))\n",
    "print(jax_array)\n",
    "\n",
    "new_jax_array = jax_array.at[::2, 3:].add(7.)\n",
    "print(\"new array post-addition:\")\n",
    "print(new_jax_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377cb590",
   "metadata": {},
   "source": [
    "Another issue with JAX's approach  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09612a33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9c4b233",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0edef76",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
